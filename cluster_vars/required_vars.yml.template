
# Cluster configs
cluster_name: "mycluster"
cloud_provider: "ec2" # Values could be "ec2", "gce", or "none"
management_user: "ubuntu"

# Amazon EC2
ec2_public_key: "<RSA public key to match your instance login private key>"
ec2_region: "us-east-1"
ec2_zone: "us-east-1a"

# Amazon Route53
# This section is only required if you want to use Route53 to manage DNS names for the cluster
# AWS default ec2 public dns names are still created
use_route53: true
route53_zone: "mydomain.com"
route53_zone_id: "<Route53 Hosted Zone ID>"
route53_region: "us-east-1"

# HAProxy service discovery rules
service_discovery_dns_suffix: "{{cluster_name}}.{{route53_zone}}"
haproxy_use_ssl: false
haproxy_certificate_path: /opt/haproxy/haproxy.pem

# This will configure HAProxy to serve out the welder service on port 80/443 of "welder.cluster_name.mydomain.com"
# Any other service running on the master you'd like to map DNS to HAProxy can be added with a {name, dns_name, port} dict.
haproxy_dns_services:
  - { app_name: 'welder', service_port: '80', url: "welder.{{service_discovery_dns_suffix}}", port: '8890' }
  - { app_name: 'welder_console', service_port: '80', url: "welder_console.{{service_discovery_dns_suffix}}", port: '8891' }
  - { app_name: 'mesos', service_port: '80', url: "mesos.{{service_discovery_dns_suffix}}", port: '5050' }
  - { app_name: 'marathon', service_port: '80', url: "marathon.{{service_discovery_dns_suffix}}", port: '8080' }
  - { app_name: 'metrics', service_port: '80', url: "metrics.{{service_discovery_dns_suffix}}", port: '3000' }
  - { app_name: 'status', service_port: '80', url: "status.{{service_discovery_dns_suffix}}", port: '18100' }
  - { app_name: 'influx', service_port: '80', url: "influx.{{service_discovery_dns_suffix}}", port: '8086' }

external_welder_name: "welder.{{service_discovery_dns_suffix}}"
external_welder_console_name: "welder_console.{{service_discovery_dns_suffix}}"

# NFS Data options
nfs_data_mount: "/data"
data_mount_point: "/mnt/data"

# Mesos Node definitions

# Service nodes serve up NFS to the cluster.  They require some sort of external mount to safely store all NFS data
# This example uses a standard SSD on EBS for EC2.
# In the NFS state, only one service node is usable
service_nodes:
  count: 1
  root_volume_size_gb: 16
  root_volume_type: "standard"
  instance_type: "t2.micro"
  additional_volumes:
    - type: "gp2"
      size_gb: 100
      device_name: "/dev/xvdf"
      mount_point: "{{ nfs_data_mount }}"

# Masters perform a lot of cluster management like hosting of frameworks and monitoring. They require medium amounts of
# memory, CPU, and disk space.
# Generally 1 master is enough to run a small cluster, but 2-3 masters will provide high-availability
# This config has not been well tested with multiple masters.
masters:
  count: 1
  root_volume_size_gb: 64
  root_volume_type: "standard"
  instance_type: "t2.medium"


# Define each kind of slave you may use in a cluster in this structure.
# If a type has a count of 0, no resources will be used by that type, and no money will be spent.
# A slave type can be dynamically scaled from 0-N nodes.
slave_types:

  # Suitable only for testing
  slave_tiny:
    count: 0
    root_volume_size_gb: 8
    root_volume_type: "standard"
    instance_type: "t2.micro"

  # Use this for small, low overhead services
  slave_small:
    count: 0
    root_volume_size_gb: 16
    root_volume_type: "standard"
    instance_type: "t2.small"

  # Reasonable for testing and some long-running services
  slave_gp:
    count: 0
    root_volume_size_gb: 16
    root_volume_type: "standard"
    instance_type: "t2.medium"

  # Slave type which will be booted by the kascaler utility
  slave_kascaler:
    count: 0
    root_volume_size_gb: 16
    root_volume_type: "standard"
    instance_type: "t2.medium"

  # This could host a decent scalable database like Cassandra
  # Note the additional SSD EBS volumes for local storage.
  slave_db:
    count: 0
    root_volume_size_gb: 16
    root_volume_type: "gp2"
    instance_type: "t2.large"
    additional_volumes:
      - type: "gp2"
        size_gb: 32
        device_name: "/dev/xvdf"
        mount_point: "/mnt/vol0"
      - type: "gp2"
        size_gb: 32
        device_name: "/dev/xvdg"
        virtual_name: "/mnt/vol1"

  # Using a larger instance with local SSD disks offers high speed and dependable resources and networking for services
  # like Kafka
  slave_kafka:
    count: 0
    root_volume_size_gb: 8
    root_volume_type: "gp2"
    instance_type: "m3.xlarge"
    instance_stores:
      - device_name: "/dev/xvdf"
        virtual_name: "ephemeral0"
        mount_point: "/mnt/vol0"
      - device_name: "/dev/xvdg"
        virtual_name: "ephemeral1"
        mount_point: "/mnt/vol1"

# Network settings
private_lan_net: "10.10.0.0/16"
private_lan_subnet: "10.10.0.0/24"
private_lan_dns: "10.10.0.2"


# Local System scaler settings
#local_slave_deploy: true
#slave_ami: ""
use_s3_remote_terraform_state: true
terraform_s3_state_bucket: "appsoma-ansible-terraform-states"

use_ansible_scaler: true
deploy_version: "HEAD"
ansible_scaler_version: "{{ deploy_version }}"

# Network settings
private_lan_net: "10.10.0.0/16"
private_lan_subnet: "10.10.0.0/24"
private_lan_dns: "10.10.0.2"

run_slave_on_master: true  # Allows jobs like frameworks to use Master resources

use_ufw: false    # Use UFW firewall rules to secure the cluster nodes
use_halo: false   # Use the CloudPassage halo endpoint security daemon
use_openvpn: true # Create an OpenVPN server on the service node

# This allows you to run an SSH on another port and allows Ansible to find it.
# alternate_ssh_port: 10203

use_kafka: true

additional_ssh_keys:
#  - "<ssh key 1>"
#  - "<ssh key 2>"

use_docker_registry: false
s3_docker_registry_store: "<S3 bucket name >"