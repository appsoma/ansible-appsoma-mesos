
# Cluster configs
cluster_name: "mycluster"
cloud_provider: "ec2" # Values could be "ec2", "gce", or "none"
management_user: "ubuntu"

# Amazon EC2
ec2_public_key: "<RSA public key to match your instance login private key>"
ec2_region: "us-east-1"
ec2_zone: "us-east-1a"

# Amazon Route53
# This section is only required if you want to use Route53 to manage DNS names for the cluster
# AWS default ec2 public dns names are still created
use_route53: true
route53_zone: "mydomain.com"
route53_zone_id: "<Route53 Hosted Zone ID>"
route53_region: "us-east-1"

# HAProxy service discovery rules
service_discovery_dns_suffix: "{{cluster_name}}.{{route53_zone}}"
haproxy_use_ssl: false
haproxy_certificate_path: /opt/haproxy/haproxy.pem

# This will configure HAProxy to serve out the welder service on port 80/443 of "welder.cluster_name.mydomain.com"
# Any other service running on the master you'd like to map DNS to HAProxy can be added with a {name, dns_name, port} dict.
haproxy_dns_services:
  - { app_name: 'welder', service_port: '80', url: "welder.{{service_discovery_dns_suffix}}", port: '8890' }
  - { app_name: 'welder_console', service_port: '80', url: "welder_console.{{service_discovery_dns_suffix}}", port: '8891' }
  - { app_name: 'mesos', service_port: '80', url: "mesos.{{service_discovery_dns_suffix}}", port: '5050' }
  - { app_name: 'marathon', service_port: '80', url: "marathon.{{service_discovery_dns_suffix}}", port: '8080' }
  - { app_name: 'metrics', service_port: '80', url: "metrics.{{service_discovery_dns_suffix}}", port: '3000' }
  - { app_name: 'status', service_port: '80', url: "status.{{service_discovery_dns_suffix}}", port: '18100' }
  - { app_name: 'influx', service_port: '80', url: "influx.{{service_discovery_dns_suffix}}", port: '8086' }

external_welder_name: "welder.{{service_discovery_dns_suffix}}"
external_welder_console_name: "welder_console.{{service_discovery_dns_suffix}}"

# Mesos Node definitions
service_nodes:
  count: 1
  root_volume_size_gb: 16
  root_volume_type: "standard"
  data_volume_type: "standard"
  data_volume_size_gb: 100
  instance_type: "t2.micro"

masters:
  count: 1
  root_volume_size_gb: 32
  root_volume_type: "standard"
  instance_type: "t2.medium"

slave_types:
  - name: "slave_small"
    count: 0
    root_volume_size_gb: 16
    root_volume_type: "standard"
    instance_type: "t2.micro"

  - name: "slave_gp"
    count: 1
    root_volume_size_gb: 16
    root_volume_type: "standard"
    instance_type: "t2.medium"

  - name: "slave_db"
    count: 0
    root_volume_size_gb: 16
    root_volume_type: "gp2"
    instance_type: "t2.large"

# Network settings
private_lan_net: "10.10.0.0/16"
private_lan_subnet: "10.10.0.0/24"
private_lan_dns: "10.10.0.2"

# NFS Data options
nfs_data_mount: "/data"
data_mount_point: "/mnt/data"

# Enable this script to burn a slave AMI.  Any AWS boot method may be used to add new slaves
# with the AMI id created (named {{cluster_name}}_slave_image).  Just use the same subnet
# and the security group {{ cluster_name }}-all-sg
# Currently for test only
create_slave_image: false

run_slave_on_master: true  # Allows jobs like frameworks to use Master resources

use_ufw: false    # Use UFW firewall rules to secure the cluster nodes
use_halo: false   # Use the CloudPassage halo endpoint security daemon
use_openvpn: true # Create an OpenVPN server on the service node

# This allows you to run an SSH on another port and allows Ansible to find it.
# alternate_ssh_port: 10203

use_kafka: true

additional_ssh_keys:
  - "<ssh key 1>"
  - "<ssh key 2>"

use_docker_registry: false
s3_docker_registry_store: "<S3 bucket name >"