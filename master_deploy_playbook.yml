---
- hosts: localhost
  connection: local
  vars_files:
    - "cluster_vars/common.yml"
    - "cluster_vars/{{ cluster_name }}/required_vars.yml"
    - "cluster_vars/{{ cluster_name }}/aws_secret_vars.yml"
  roles:
    - role: ../roles/ansible-cloud-inventory-parse
      provider: "{{ cloud_provider }}"

# Check for SSH port function
- hosts: [ "{{ inventory_group_master }}" ]
  remote_user: "{{ management_user }}"
  gather_facts: no
  vars_files:
    - "cluster_vars/common.yml"
    - "cluster_vars/{{ cluster_name }}/required_vars.yml"
  roles:
    - role: ansible-find-ssh
      when: alternate_ssh_port is defined

- hosts: localhost
  connection: local
  vars_files:
    - "cluster_vars/common.yml"
    - "cluster_vars/{{ cluster_name }}/required_vars.yml"
    - "cluster_vars/{{ cluster_name }}/aws_secret_vars.yml"
  tasks:
    - name: Create Service Discovery subnet wildcard DNS record
      route53:
        command: create
        overwrite: yes
        type: CNAME
        zone: "{{ route53_zone }}"
        ttl: 10
        record: "{{ service_discovery_subnet }}.{{ route53_zone }}"
        value: "{{ hostvars[groups[inventory_group_master][0]]['public_dns_name'] }}"
      when: use_service_discovery_subnet is defined and use_service_discovery_subnet and use_route53 is defined and use_route53

    - name: Create Master DNS entries
      route53:
        command: create
        overwrite: yes
        type: CNAME
        zone: "{{ route53_zone }}"
        ttl: 30
        record: "{{ hostvars[item].short_name }}.{{ service_discovery_dns_suffix }}"
        value: "{{ hostvars[item].public_dns_name }}"
      with_items: groups[inventory_group_master]
      when: use_service_discovery_subnet is defined and use_service_discovery_subnet and use_route53 is defined and use_route53

- hosts: [ "{{ inventory_group_master }}" ]
  remote_user: "{{ management_user }}"
  sudo: True
  vars_files:
    - "cluster_vars/common.yml"
    - "cluster_vars/{{ cluster_name }}/required_vars.yml"
    - "cluster_vars/{{ cluster_name }}/users.yml"
  pre_tasks:

    - name: Add AWS vars
      include_vars: "cluster_vars/{{ cluster_name }}/aws_secret_vars.yml"
      no_log: true
      when: "(cloud_provider is defined and cloud_provider == 'ec2') or (use_docker_registry is defined and use_docker_registry)"

    - name: Find NFS Server IP
      set_fact: nfsServerIp={{ hostvars[groups[inventory_group_service][0]]['private_ip_address'] }}
      when: groups[inventory_group_service] is defined

    - name: Set zookeeper private IP list with port
      set_fact:
        zookeeper_hosts_with_port: |
          {% set comma = joiner(':' + zookeeper_client_port + ',') %}
          {% for item in groups[inventory_group_master] -%}
            {{ comma() }}{{ hostvars[item].private_ip_address }}:{{ zookeeper_client_port }}
          {%- endfor %}

    - name: Save zookeeper private IP list to bash profile
      lineinfile: dest=/etc/profile.d/zookeeper_host_list.sh line="export ZK_HOST_LIST={{ zookeeper_hosts_with_port }}" state=present create=yes

    - name:  Set zookeeper private IP list
      set_fact:
        zookeeper_hosts_no_port: |
          {% set comma = joiner(',') %}
          {% for item in groups[inventory_group_master] -%}
            {{ comma() }}{{ hostvars[item].private_ip_address }}
          {%- endfor %}

    - name: Set Mesos master run mode
      set_fact: mesos_mode="{{ 'master-slave' if run_slave_on_master is defined and run_slave_on_master else 'master' }}"

    - name: Add ssh keys to management user
      authorized_key: key="{{ item }}" state=present user=ubuntu exclusive=no
      with_items: additional_ssh_keys
      when: additional_ssh_keys is defined

  roles:
    # Install NTP for time sync
    - role: ansible-ntp

    # Install sysdig for troubleshooting
    - role: ansible-sysdig

    # Install CloudPassage Halo agent if available
    - role: ansible-cphalo
      halo_agent_key: "{{ cp_agent_key }}"
      halo_agent_tag: "{{ cp_agent_tag }}"
      when: use_halo is defined and use_halo

    # Add UFW firewall if selected (should not be used with halo)
    - role: ansible-ufw
      extra_ports: "{{ master_firewall_ports }}"
      when: use_ufw is defined and use_ufw

    # Install Docker
    - role: ansible-docker
      when: use_docker_registry is not defined or not use_docker_registry

    - role: ansible-docker
      registry_host: "master.mesos"
      marathon_host: "{{ hostvars[groups[inventory_group_master][0]].private_ip_address }}"
      when: use_docker_registry is defined and use_docker_registry

    # Install Python
    - role: ansible-python

    # Install NodeJs
    - role: ansible-nvm-nodejs
      nodejs_version: "v0.12.4"
      nodejs_global_packages:
        - browserify
        - mocha
        - nodejs-websocket
        - phantomjs
        - process
        - require
        - scribe-js
        - supervisor
      tags: ["nodejs"]

    # Configure NFS
    - role: ansible-nfs-client
      nfsmount: "{{ data_mount_point }}"
      nfspath: "{{ nfs_data_mount }}"
      nfsserver: "{{ nfsServerIp }}"
      when: nfsServerIp is defined

    # Create an users
    - role: ansible-welder-user
      group: "{{ welder_group }}"
      user_list: "{{ welder_user_list }}"
      dataDir: "{{ data_mount_point }}"

    # Make sure we have usable JDKs on masters
    - role: ansible-java
      java_versions: [6, 7, 8]

    - role: ansible-go

    - role: ../roles/ansible-maven

    # Install zookeeper
    - role: ansible-zookeeper
      zookeeper_hosts: "{{ groups[inventory_group_master] }}"
      zookeeper_id: "{{ hostvars[inventory_hostname]['zoo_id'] }}"

    # Install High Availability Proxy
    - role: ../roles/ansible-haProxy
      zookeeper_host_list: "{{zookeeper_hosts_with_port}}"
      haproxy_pem_file: "cluster_vars/{{ cluster_name }}/haproxy.pem"

    # Install Mesos
    - role: ansible-mesos
      zookeeper_hostnames: "{{ zookeeper_hosts_with_port  }}"
      mesos_install_mode: "{{ mesos_mode }}"
      mesos_cluster_name: "{{cluster_name}}"
      mesos_hostname: "{{ hostvars[inventory_hostname]['private_ip_address'] }}"

    # Install Marathon
    - role: ansible-marathon
      zookeeper_hostnames: "{{ zookeeper_hosts_with_port  }}"
      when: inventory_hostname == groups[inventory_group_master][0]

    # Install Appsoma Rhino
    - role: ../roles/ansible-rhino
      zookeeper_hostnames: "{{ zookeeper_hosts_with_port }}"
      db_folder: "{{ data_mount_point }}/rhino_db"
      when: inventory_hostname == groups[inventory_group_master][0]

    - role: ansible-mesos-dns
      zookeeper_host_list: "{{ zookeeper_hosts_with_port }}"
      marathon_host: "{{ hostvars[groups[inventory_group_master][0]].private_ip_address }}"
      mesos_dns_hosts: [ "{{ hostvars[groups[inventory_group_master][0]].private_ip_address }}" ]
      is_mesos_dns_starter: true

    - role: ansible-cassandra-marathon
      marathon_host: "{{ hostvars[groups[inventory_group_master][0]].private_ip_address }}"

    - role: ansible-rabbitmq

    - role: ansible-redis

    - role: ansible-influxdb

    - role: ansible-grafana

    - role: ansible-sensu
      sensu_mode: "master"
      sensu_master: "{{ hostvars[groups[inventory_group_master][0]].private_ip_address }}"
      marathon_host: "{{ hostvars[groups[inventory_group_master][0]].private_ip_address }}"
      mesos_master_list: "{{ zookeeper_hosts_no_port }}"

    - role: ../roles/ansible-kafka-mesos
      zookeeper_hosts: "{{ zookeeper_hosts_with_port }}"
      zookeeper_hosts_no_port_list: "{{ zookeeper_hosts_no_port.split(',') }}"
      framework_host: "{{ hostvars[inventory_hostname]['private_dns_name'] }}"
      when: "use_kafka is defined and use_kafka and inventory_hostname == groups[inventory_group_master][0]"

    - role: ../roles/ansible-storm-mesos
      zookeeper_hosts: "{{ zookeeper_hosts_with_port }}"
      zookeeper_hosts_no_port_list: "{{ zookeeper_hosts_no_port.split(',') }}"
      framework_host: "{{ hostvars[inventory_hostname]['private_dns_name'] }}"
      when: "use_storm is defined and use_storm and inventory_hostname == groups[inventory_group_master][0]"

- include: playbooks/welder_playbook.yml

